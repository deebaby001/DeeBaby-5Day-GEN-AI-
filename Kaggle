{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":97258,"sourceType":"competition"},{"sourceId":11347970,"sourceType":"datasetVersion","datasetId":7100433},{"sourceId":11366230,"sourceType":"datasetVersion","datasetId":7114637},{"sourceId":11386442,"sourceType":"datasetVersion","datasetId":7130060}],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/donnahardy/deebaby-genai-5day-proj-v2?scriptVersionId=234538266\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"# Agricultural Yield Forecasting: An Investigation of the use of GEN AI to evaluate Geographical Imagery","metadata":{}},{"cell_type":"code","source":"from IPython.display import Image\n\n# Replace 'path_to_image' with the actual path to your uploaded image\nImage(filename='/kaggle/input/main-satellite-image/satelite-imagery-in-farming_farm-management_forestry-management.jpg')\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T11:23:51.068128Z","iopub.status.idle":"2025-04-15T11:23:51.068472Z","shell.execute_reply":"2025-04-15T11:23:51.068327Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"/kaggle/input/main-satellite-image/satelite-imagery-in-farming_farm-management_forestry-management.jpg","metadata":{}},{"cell_type":"markdown","source":"# STATEMENT OF PURPOSE","metadata":{}},{"cell_type":"markdown","source":"STATEMENT of PURPOSE: Agricultural Satellite imagery plays a crucial role in achieving the United Nations Sustainable Development Goal 2: Zero Hunger. This goal is attained by enabling farmers and researchers to efficiently monitor large agricultural areas. This technology helps optimize crop management, improve yield predictions, and ensure food security on a large multi-regional scale.","metadata":{}},{"cell_type":"markdown","source":"# USE CASE INFO","metadata":{}},{"cell_type":"markdown","source":"USE CASE TOPIC:  Agricultural Yield Forecasting\n\nINTENT: The intention of this project is to evaluate the United Nations Sustainable Development Goal #2: Zero Hunger, to utilize satellite imagery and weather data to predict crop yields and optimize food distribution. For the purpose of this project I will evaluate 1 satellite image and use the following Generative AI technique 1. Embeddings & Vector Search/Store  2.  Few Shot Prompting and 3. Image Understanding to evaluate the use of the satellite image. ","metadata":{}},{"cell_type":"markdown","source":"# Environment Set-Up\n\nPurpose: The environment will be utilized for this project. A listing of the primary applications and libraries will follow.\n\n**NOTE: A Timing Parameter must need to be set prior to an Auto Run of this notebook due to the complexity of the code Theree could be errors to result if the timing is not adjusted. Resorce allocation and server avaiability may also be a factor**","metadata":{}},{"cell_type":"markdown","source":"# Libraries and Applications Utilized\n\nLibraries:\n1.\tIPython.display: Used for displaying images and other outputs.\n2.\tgoogle-genai: Installed and used for Generative AI capabilities.\n3.\tgoogle.genai: Imported for specific functions and types.\n\nLibraries Explained:\n•\tgoogle.generativeai (or genai): Essential for interacting with Google's Gemini models for text generation, chat, or multimodal tasks.\n•\tos: Used to manage environment variables, especially for storing API keys securely.\n•\tPIL (Pillow): Used for image processing, such as loading, resizing, or converting image formats.\n•\trequests: Used for making network requests, potentially for fetching images from URLs.\n•\tio: Might be used for working with image data in memory.\n•\tre: Might be used for text processing or pattern matching within text prompts.\n\nApplications:\n1.\tEmbeddings & Vector Search/Store: Utilized for evaluating satellite images.\n2.\tFew Shot Prompting: Applied for generating AI responses.\n3.\tImage Understanding: Used to analyze and interpret satellite imagery.\n  \nApplications Explained:\n•\tPrompt Engineering: Crafting specific instructions for the generative models.\n•\tGemini API: Using the Gemini model through the API to perform tasks.\n•\tAPI Key Management: Handling the Google Cloud API key.\n•\tImage Handling: Loading, processing, and working with images for multimodal prompts.\n•\tText Processing: Cleaning and preparing text for the LLM.\n•\tPotentially, Retrieval Augmented Generation (RAG): If the project aims to improve accuracy or reduce hallucinations.\n","metadata":{}},{"cell_type":"markdown","source":"Install SDK","metadata":{}},{"cell_type":"code","source":"!pip uninstall -qy jupyterlab  # Remove unused packages from Kaggle's base image that conflict\n!pip install -U -q \"google-genai==1.7.0\"\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T11:30:10.913992Z","iopub.execute_input":"2025-04-15T11:30:10.914412Z","iopub.status.idle":"2025-04-15T11:30:19.921247Z","shell.execute_reply.started":"2025-04-15T11:30:10.914352Z","shell.execute_reply":"2025-04-15T11:30:19.91987Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Installing additional code to support the SDK","metadata":{}},{"cell_type":"code","source":"from google import genai\nfrom google.genai import types\nfrom IPython.display import HTML, Markdown, display\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T11:31:08.640037Z","iopub.execute_input":"2025-04-15T11:31:08.640428Z","iopub.status.idle":"2025-04-15T11:31:09.756192Z","shell.execute_reply.started":"2025-04-15T11:31:08.640362Z","shell.execute_reply":"2025-04-15T11:31:09.754966Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Set Up the API Key","metadata":{}},{"cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\n\nGOOGLE_API_KEY = UserSecretsClient().get_secret(\"GOOGLE_API_KEY\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T11:31:22.161547Z","iopub.execute_input":"2025-04-15T11:31:22.161921Z","iopub.status.idle":"2025-04-15T11:31:22.347881Z","shell.execute_reply.started":"2025-04-15T11:31:22.161892Z","shell.execute_reply":"2025-04-15T11:31:22.346539Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Summary of elements installed\n\nAdded image in cover for notebook.\n\nDefined the API key and confirmed it is set up correctly.\n\nInstallation of varous libaries and models for use within the notebook. For example: BertModel, VisionEncoderDecoderModel and CLIPProcessor and others.\n","metadata":{}},{"cell_type":"markdown","source":"# GEN AI CAPABILITIES EXEMPLIFIED","metadata":{}},{"cell_type":"markdown","source":"# EMBEDDING AND VECTOR SEARCH ( 1st Example)\n","metadata":{}},{"cell_type":"markdown","source":"Objective:\n\nLoading and Preprocessing the Image: The code loads an image using the PIL library and preprocesses it with ViTFeatureExtractor to prepare it for caption generation.\n\nGenerating a Caption for the Image: It uses the VisionEncoderDecoderModel to generate a caption based on the preprocessed image, leveraging a pre-trained model on image-caption pairs.\n\nDecoding and Printing the Generated Caption: The AutoTokenizer decodes the model's output into a human-readable caption, which is then printed to provide a textual description of the image.\n\n\n","metadata":{}},{"cell_type":"code","source":"from transformers import BertModel, BertTokenizer\n\ndef encode_question(question):\n    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n    model = BertModel.from_pretrained('bert-base-uncased')\n    inputs = tokenizer(question, return_tensors='pt')\n    outputs = model(**inputs)\n    return outputs.last_hidden_state\n\ndef link_question_to_graph(question_embedding, node_embeddings):\n    similarities = torch.nn.functional.cosine_similarity(question_embedding, node_embeddings)\n    relevant_nodes = torch.argmax(similarities, dim=1)\n    return relevant_nodes\n\ndef retrieve_answer(relevant_nodes, graph_data):\n    answers = []\n    for node in relevant_nodes:\n        answers.append(graph_data[node])\n    return answers\n\ndef display_answer(answer):\n    display(Markdown(answer))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T11:31:43.909931Z","iopub.execute_input":"2025-04-15T11:31:43.910294Z","iopub.status.idle":"2025-04-15T11:31:43.948591Z","shell.execute_reply.started":"2025-04-15T11:31:43.910268Z","shell.execute_reply":"2025-04-15T11:31:43.947155Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#Access Uploaded Image in Notebook","metadata":{}},{"cell_type":"code","source":"#Install Rasterio\n!pip install rasterio\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T11:34:14.350558Z","iopub.execute_input":"2025-04-15T11:34:14.350944Z","iopub.status.idle":"2025-04-15T11:34:20.475464Z","shell.execute_reply.started":"2025-04-15T11:34:14.350919Z","shell.execute_reply":"2025-04-15T11:34:20.473949Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Install Numpy\n!pip install numpy\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T11:34:41.426889Z","iopub.execute_input":"2025-04-15T11:34:41.427246Z","iopub.status.idle":"2025-04-15T11:34:45.840806Z","shell.execute_reply.started":"2025-04-15T11:34:41.427217Z","shell.execute_reply":"2025-04-15T11:34:45.839478Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from PIL import Image\n\n# Open the JPEG image\njpeg_image_path = '/kaggle/input/brazil-agriculture/MYD13A2_V061_04Aug2020_Brazil_HERO.original.jpg'\nimage = Image.open(jpeg_image_path)\n\n# Save the image as a TIFF in a writable directory\ntif_image_path = '/kaggle/working/Brazil.tiff'\nimage.save(tif_image_path, format='TIFF')\n\nprint(f\"Image converted and saved as {tif_image_path}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T11:36:24.467277Z","iopub.execute_input":"2025-04-15T11:36:24.46867Z","iopub.status.idle":"2025-04-15T11:36:24.508817Z","shell.execute_reply.started":"2025-04-15T11:36:24.46863Z","shell.execute_reply":"2025-04-15T11:36:24.507677Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import rasterio\nimport numpy as np\n\n# Load satellite image\ndef load_image(file_path):\n    with rasterio.open(file_path) as src:\n        image = src.read()\n    return image\n\n# Example usage\nfile_path = '/kaggle/working/Brazil.tiff'\nimage = load_image(file_path)\n\n# Display basic information about the image\nprint(f\"Image shape: {image.shape}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T11:36:30.384304Z","iopub.execute_input":"2025-04-15T11:36:30.384753Z","iopub.status.idle":"2025-04-15T11:36:30.397719Z","shell.execute_reply.started":"2025-04-15T11:36:30.384724Z","shell.execute_reply":"2025-04-15T11:36:30.396548Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#Preprocessing ","metadata":{}},{"cell_type":"code","source":"import rasterio\nfrom rasterio.enums import Resampling\nimport numpy as np\nfrom skimage import exposure, segmentation\n\n# Geometric Correction and Registration\ndef geometric_correction(image_path):\n    with rasterio.open(image_path) as src:\n        transform = src.transform\n        profile = src.profile\n        data = src.read(\n            out_shape=(\n                src.count,\n                int(src.height * 2),\n                int(src.width * 2)\n            ),\n            resampling=Resampling.bilinear\n        )\n        profile.update(transform=transform, height=data.shape[1], width=data.shape[2])\n        corrected_image_path = '/kaggle/working/Brazil.tiff'\n        with rasterio.open(corrected_image_path, 'w', **profile) as dst:\n            dst.write(data)\n    return corrected_image_path\n\n# Atmospheric Correction\ndef atmospheric_correction(image):\n    corrected_image = image - np.mean(image, axis=0)\n    return corrected_image\n\n# Cloud Removal\ndef cloud_removal(image):\n    cloud_mask = image < np.percentile(image, 90)\n    image[cloud_mask] = np.nan\n    return image\n\n# Normalization/Standardization\ndef normalize_image(image):\n    normalized_image = exposure.rescale_intensity(image, out_range=(0, 1))\n    return normalized_image\n\n# Image Segmentation\ndef segment_image(image):\n    segments = segmentation.slic(image, n_segments=100, compactness=10)\n    return segments\n\n# Create Bounding Boxes/Polygons\ndef create_bounding_boxes(segments):\n    bounding_boxes = []\n    for segment in np.unique(segments):\n        mask = segments == segment\n        coords = np.column_stack(np.where(mask))\n        bounding_boxes.append(coords)\n    return bounding_boxes\n\n# Example preprocessing workflow\nimage_path = '/kaggle/working/Brazil.tiff'\ncorrected_image_path = geometric_correction(image_path)\nwith rasterio.open(corrected_image_path) as src:\n    corrected_image = src.read(1)\nnormalized_image = normalize_image(corrected_image)\nsegments = segment_image(normalized_image)\nbounding_boxes = create_bounding_boxes(segments)\n\nprint(f\"Number of segments: {len(bounding_boxes)}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T11:36:37.789918Z","iopub.execute_input":"2025-04-15T11:36:37.7903Z","iopub.status.idle":"2025-04-15T11:36:38.393046Z","shell.execute_reply.started":"2025-04-15T11:36:37.790268Z","shell.execute_reply":"2025-04-15T11:36:38.39141Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install transformers torch pillow","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T11:37:46.163361Z","iopub.execute_input":"2025-04-15T11:37:46.1638Z","iopub.status.idle":"2025-04-15T11:37:50.647585Z","shell.execute_reply.started":"2025-04-15T11:37:46.163767Z","shell.execute_reply":"2025-04-15T11:37:50.646113Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":" from PIL import Image\n from transformers import VisionEncoderDecoderModel, ViTFeatureExtractor, AutoTokenizer\n\n # Load the image\n image_path = '/kaggle/working/Brazil.tiff'  # Update this with the correct path\n image = Image.open(image_path)\n\n # Load the model and feature extractor\n model = VisionEncoderDecoderModel.from_pretrained(\"nlpconnect/vit-gpt2-image-captioning\")\n feature_extractor = ViTFeatureExtractor.from_pretrained(\"nlpconnect/vit-gpt2-image-captioning\")\n tokenizer = AutoTokenizer.from_pretrained(\"nlpconnect/vit-gpt2-image-captioning\")\n\n # Preprocess the image\n pixel_values = feature_extractor(images=image, return_tensors=\"pt\").pixel_values\n\n # Generate a caption\n outputs = model.generate(pixel_values)\n caption = tokenizer.decode(outputs[0], skip_special_tokens=True)\n\n print(f\"Generated Caption: {caption}\")\n ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T11:38:39.310893Z","iopub.execute_input":"2025-04-15T11:38:39.311251Z","iopub.status.idle":"2025-04-15T11:38:47.815756Z","shell.execute_reply.started":"2025-04-15T11:38:39.311224Z","shell.execute_reply":"2025-04-15T11:38:47.81458Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"NOTE; There are two (2) paths that are being used in the following: \n\nPath #1.\n\nVisualBertModel and BertTokenizer from the transformers library.\ntorch from the torch library.\nLoads the Model and Tokenizer:\n\nLoads the VisualBertModel pre-trained on the Visual Question Answering (VQA) task using the uclanlp/visualbert-vqa-coco-pre model.\nLoads the BertTokenizer pre-trained on the bert-base-uncased model.\nDefines the Question and Context:\n\nSets the question to \"What is in the image?\".\nUses a variable caption as the context, which should contain the generated caption for the image.\nTokenizes the Input:\n\nTokenizes the question and context into tensors suitable for the model using the tokenizer.\nPerforms a Forward Pass:\n\nPasses the tokenized inputs through the model to get the outputs.\nExtracts Logits:\n\nExtracts the logits from the model outputs.\nGets the Answer:\n\nAssumes a binary classification task and uses torch.argmax to get the index of the highest logit value, which is interpreted as the answer.\nPrints the Answer:\n\nPrints the answer.\n\nPath #2 \n","metadata":{}},{"cell_type":"code","source":" from transformers import VisualBertModel, BertTokenizer\n import torch\n\n # Load the model and tokenizer\n model = VisualBertModel.from_pretrained(\"uclanlp/visualbert-vqa-coco-pre\")\n tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n\n # Define the question and the caption (context)\n question = \"What is in the image?\"\n context = caption  # Use the generated caption as context\n\n # Tokenize the input\n inputs = tokenizer(question, context, return_tensors=\"pt\")\n\n # Forward pass\n outputs = model(**inputs)\n logits = outputs.logits\n\n # Get the answer (assuming binary classification for simplicity)\n answer = torch.argmax(logits, dim=-1).item()\n\n print(f\"Answer: {answer}\")\n ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T11:39:01.915756Z","iopub.execute_input":"2025-04-15T11:39:01.916185Z","iopub.status.idle":"2025-04-15T11:39:06.615509Z","shell.execute_reply.started":"2025-04-15T11:39:01.91615Z","shell.execute_reply":"2025-04-15T11:39:06.613151Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Path# 2\n\nPath # 2 Part 1. Forward Pass\n\noutputs = model(**inputs): This line passes the inputs through the model. The inputs are typically tokenized data that the model processes to generate outputs. The **inputs syntax unpacks the dictionary of inputs, passing them as keyword arguments to the model.\nPrint Outputs:\n\nprint(outputs): This line prints the outputs object to the console. This is useful for inspecting the structure and contents of the outputs to understand what attributes and data it contains.\n\n(In the subsequent code block, the code is performing the following:)\n\nPath # 2. Part 2 Imports Required Libraries for the VisualBertModel and BertTokenizer \n\nVisualBertModel and BertTokenizer from the transformers library.\ntorch from the torch library.\nLoads the Model and Tokenizer:\n\nLoads the VisualBertModel pre-trained on the Visual Question Answering (VQA) task using the uclanlp/visualbert-vqa-coco-pre model.\nLoads the BertTokenizer pre-trained on the bert-base-uncased model.\nDefines the Question and Context:\n\nSets the question to \"What is in the image?\".\nUses a variable caption as the context, which should contain the generated caption for the image.\nTokenizes the Input:\n\nTokenizes the question and context into tensors suitable for the model using the tokenizer.\nPerforms a Forward Pass:\n\nPasses the tokenized inputs through the model to get the outputs.\nChecks and Extracts Logits:\n\nChecks if the outputs object has the attribute logits. If it does, it extracts logits.\nIf logits do not exist, it uses pooler_output or last_hidden_state as an alternative.\nGets the Answer:\n\nAssumes a binary classification task and uses torch.argmax to get the index of the highest logit value, which is interpreted as the answer.\nPrints the Answer:\n\nPrints the answer.\n\n","metadata":{}},{"cell_type":"code","source":"#Path # 2 Part 1. Forward Pass\n# Forward pass\noutputs = model(**inputs)\n\n# Print the outputs to check its structure\nprint(outputs)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T11:56:03.44712Z","iopub.execute_input":"2025-04-15T11:56:03.448649Z","iopub.status.idle":"2025-04-15T11:56:03.594191Z","shell.execute_reply.started":"2025-04-15T11:56:03.448563Z","shell.execute_reply":"2025-04-15T11:56:03.59262Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Path # 2. Part 2 Imports Required Libraries for the VisualBertModel and BertTokenizer \nfrom transformers import VisualBertModel, BertTokenizer\nimport torch\n\n# Load the model and tokenizer\nmodel = VisualBertModel.from_pretrained(\"uclanlp/visualbert-vqa-coco-pre\")\ntokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n\n# Define the question and the caption (context)\nquestion = \"What is in the image?\"\ncontext = caption  # Use the generated caption as context\n\n# Tokenize the input\ninputs = tokenizer(question, context, return_tensors=\"pt\")\n\n# Forward pass\noutputs = model(**inputs)\n\n# Check if 'logits' exist, otherwise use 'pooler_output' or 'last_hidden_state'\nif hasattr(outputs, 'logits'):\n    logits = outputs.logits\nelse:\n    logits = outputs.pooler_output  # or outputs.last_hidden_state\n\n# Get the answer (assuming binary classification for simplicity)\nanswer = torch.argmax(logits, dim=-1).item()\n\nprint(f\"Answer: {answer}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T11:56:19.693989Z","iopub.execute_input":"2025-04-15T11:56:19.694423Z","iopub.status.idle":"2025-04-15T11:56:22.531133Z","shell.execute_reply.started":"2025-04-15T11:56:19.694393Z","shell.execute_reply":"2025-04-15T11:56:22.529438Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from PIL import Image\nfrom transformers import VisionEncoderDecoderModel, ViTFeatureExtractor, AutoTokenizer, VisualBertModel, BertTokenizer\nimport torch\n\n# Load the image\nimage_path = '/kaggle/working/Brazil.tiff'  # Update this with the correct path\nimage = Image.open(image_path)\n\n# Load the image captioning model and feature extractor\ncaption_model = VisionEncoderDecoderModel.from_pretrained(\"nlpconnect/vit-gpt2-image-captioning\")\nfeature_extractor = ViTFeatureExtractor.from_pretrained(\"nlpconnect/vit-gpt2-image-captioning\")\ncaption_tokenizer = AutoTokenizer.from_pretrained(\"nlpconnect/vit-gpt2-image-captioning\")\n\n# Preprocess the image for captioning\npixel_values = feature_extractor(images=image, return_tensors=\"pt\").pixel_values\n\n# Generate a caption\ncaption_outputs = caption_model.generate(pixel_values)\ncaption = caption_tokenizer.decode(caption_outputs[0], skip_special_tokens=True)\n\nprint(f\"Generated Caption: {caption}\")\n\n# Load the VisualBERT model and tokenizer for question answering\nqa_model = VisualBertModel.from_pretrained(\"uclanlp/visualbert-vqa-coco-pre\")\nqa_tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n\n# Define the question and the caption (context)\nquestion = \"What is in the image?\"\ncontext = caption  # Use the generated caption as context\n\n# Tokenize the input for question answering\ninputs = qa_tokenizer(question, context, return_tensors=\"pt\")\n\n# Forward pass for question answering\noutputs = qa_model(**inputs)\n\n# Access the last hidden state\nlast_hidden_state = outputs.last_hidden_state\n\n# Process the last hidden state to get the answer (example: using a simple linear layer)\nlinear_layer = torch.nn.Linear(last_hidden_state.size(-1), 2)  # Assuming binary classification\nlogits = linear_layer(last_hidden_state[:, 0, :])  # Use the first token's representation\n\n# Get the answer\nanswer = torch.argmax(logits, dim=-1).item()\n\nprint(f\"Answer: {answer}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T12:15:14.302764Z","iopub.execute_input":"2025-04-15T12:15:14.30336Z","iopub.status.idle":"2025-04-15T12:15:27.863703Z","shell.execute_reply.started":"2025-04-15T12:15:14.303316Z","shell.execute_reply":"2025-04-15T12:15:27.862055Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# FEW- SHOT PROMPTING CAPABILITY ( 2nd Example)","metadata":{}},{"cell_type":"markdown","source":"Objective\n\nDefine the Task: Define the task you want the model to perform. For this example I will be answering questions.\n\nProvide Examples: Give the model a few examples of the task. These examples should include both the input and the desired output. The more relevant and clear the examples, the better the model will understand the task.\n\nConstruct the Prompt: Combine the task definition and examples into a single prompt. This prompt will guide the model in generating the desired output.\n","metadata":{}},{"cell_type":"code","source":"from transformers import VisionEncoderDecoderModel, ViTFeatureExtractor, AutoTokenizer\nfrom PIL import Image\n\n# Load the image captioning model and feature extractor\nmodel = VisionEncoderDecoderModel.from_pretrained(\"nlpconnect/vit-gpt2-image-captioning\")\nfeature_extractor = ViTFeatureExtractor.from_pretrained(\"nlpconnect/vit-gpt2-image-captioning\")\ntokenizer = AutoTokenizer.from_pretrained(\"nlpconnect/vit-gpt2-image-captioning\")\n\n# Define the few-shot examples\nexamples = [\n    (\"/kaggle/working/Brazil.tiff\", \"An image of Agricultural Vegitation.\"),\n    (\"/kaggle/working/Brazil.tiff\", \"An image of areas of dense vegitation.\"),\n    (\"/kaggle/working/Brazil.tiff\", \"An image of areas of sparse vegitation.\")\n]\n\n# Function to generate captions using few-shot prompting\ndef generate_caption(image_path):\n    image = Image.open(image_path)\n    pixel_values = feature_extractor(images=image, return_tensors=\"pt\").pixel_values\n    outputs = model.generate(pixel_values)\n    caption = tokenizer.decode(outputs[0], skip_special_tokens=True)\n    return caption\n\n# Generate captions for the examples\nfor image_path, true_caption in examples:\n    generated_caption = generate_caption(image_path)\n    print(f\"Image: {image_path}\")\n    print(f\"True Caption: {true_caption}\")\n    print(f\"Generated Caption: {generated_caption}\")\n    print()\n\n# Generate a caption for a new image\nnew_image_path = \"/kaggle/working/Brazil.tiff\"\nnew_caption = generate_caption(new_image_path)\nprint(f\"Generated Caption for New Image: {new_caption}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T12:20:53.813066Z","iopub.execute_input":"2025-04-15T12:20:53.813636Z","iopub.status.idle":"2025-04-15T12:21:08.83754Z","shell.execute_reply.started":"2025-04-15T12:20:53.813597Z","shell.execute_reply":"2025-04-15T12:21:08.835684Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# IMAGE UNDERSTANDING ( 3rd Example)\n","metadata":{}},{"cell_type":"markdown","source":"Objective:\n\nCLIP Model: The code uses the CLIP model to understand the image and match it with textual descriptions.\n\nText Prompts: I provided 2 text prompts that describe possible contents of the image.\n\nImage Processing: The image and text prompts are processed and fed into the CLIP  model.\n\nSimilarity Scores: The model outputs similarity scores between the image and each text prompt, which are converted to probabilities.\n\n\n","metadata":{}},{"cell_type":"code","source":" import torch\n from PIL import Image\n from transformers import CLIPProcessor, CLIPModel\n\n # Load the CLIP model and processor\n model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n\n # Load the image\n image_path = '/kaggle/input/ndvi-map-image-understanding/agri- image 2.png'\n image = Image.open(image_path)\n\n # Define the text prompts\n prompts = [\"A satellite agriculture image\", \"A image of dense vegitation\"]\n\n # Preprocess the image and text\n inputs = processor(text=prompts, images=image, return_tensors=\"pt\", padding=True)\n\n # Forward pass\n outputs = model(**inputs)\n\n # Get the logits\n logits_per_image = outputs.logits_per_image  # Image-to-text similarity scores\n probs = logits_per_image.softmax(dim=1)  # Convert logits to probabilities\n\n # Print the probabilities for each prompt\n for prompt, prob in zip(prompts, probs[0]):\n     print(f\"Prompt: {prompt}, Probability: {prob.item()}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T12:21:48.68707Z","iopub.execute_input":"2025-04-15T12:21:48.687575Z","iopub.status.idle":"2025-04-15T12:21:56.041927Z","shell.execute_reply.started":"2025-04-15T12:21:48.687534Z","shell.execute_reply":"2025-04-15T12:21:56.039456Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# REFERENCES\n\n*Kaggle company and Support Staff, 5-Day GEN AI Instructors, GEN AI Teams and Support Staff*\nGoogle Resources and Support: Google AI Studio, Notebook ML: https://notebooklm.google.com/ \nGoogle Earth Dataset: *https://lpdaac.usgs.gov/products/myd13a2v061/ The MYD13a2v061 Data Set*\nUnited Nations | Big Data for Sustainable Development: *https://www.un.org/en/global-issues/big-data-for-sustainable-development*\n","metadata":{}},{"cell_type":"markdown","source":"# SUMMARY\n\n\n","metadata":{}},{"cell_type":"markdown","source":"# Closing\n\nI learned a lot doing this capstone. I still have a lot to learn but I am trying.\n\nThank You Kaggle for this opportinity!\n\nDH","metadata":{}}]}