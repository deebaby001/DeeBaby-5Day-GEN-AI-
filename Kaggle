{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":97258,"sourceType":"competition"},{"sourceId":11347970,"sourceType":"datasetVersion","datasetId":7100433},{"sourceId":11366230,"sourceType":"datasetVersion","datasetId":7114637},{"sourceId":11386442,"sourceType":"datasetVersion","datasetId":7130060}],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/donnahardy/deebaby-genai-5day-proj-v2?scriptVersionId=233609191\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"# Agricultural Yield Forcasting: An Investigation of the use of GEN AI to evaluate Geographical Imagery","metadata":{}},{"cell_type":"code","source":"from IPython.display import Image\n\n# Replace 'path_to_image' with the actual path to your uploaded image\nImage(filename='/kaggle/input/main-satellite-image/satelite-imagery-in-farming_farm-management_forestry-management.jpg')\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T09:42:39.956219Z","iopub.execute_input":"2025-04-13T09:42:39.956562Z","iopub.status.idle":"2025-04-13T09:42:39.999188Z","shell.execute_reply.started":"2025-04-13T09:42:39.956527Z","shell.execute_reply":"2025-04-13T09:42:39.99772Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"/kaggle/input/main-satellite-image/satelite-imagery-in-farming_farm-management_forestry-management.jpg","metadata":{}},{"cell_type":"markdown","source":"# STATEMENT OF PURPOSE","metadata":{}},{"cell_type":"markdown","source":"STATEMENT of PURPOSE: Agricultural Satellite imagery plays a crucial role in achieving the United Nations Sustainable Development Goal 2: Zero Hunger. This goal is attained by enabling farmers and researchers to efficiently monitor large agricultural areas. This technology helps optimize crop management, improve yield predictions, and ensure food security on a large multi-regional scale.","metadata":{}},{"cell_type":"markdown","source":"# USE CASE INFO","metadata":{}},{"cell_type":"markdown","source":"USE CASE TOPIC:  Agricultural Yield Forecasting\n\nINTENT: The intention of this project is to evaluate the United Nations Sustainable Development Goal #2: Zero Hunger, to utilize satellite imagery and weather data to predict crop yields and optimize food distribution. For the purpose of this project I will evaluate 1 satellite image and use the following Generative AI technique 1. Embeddings & Vector Search/Store  2.  Few Shot Prompting and 3. Image Understanding to evaluate the use of the satellite image. Additionally, a You Tube video will be provided for addition support to this initiative.\n","metadata":{}},{"cell_type":"markdown","source":"# Environment Set-Up\n\nDefineing all applications and libraries that will be used in this project.\n","metadata":{}},{"cell_type":"markdown","source":"Install SDK","metadata":{}},{"cell_type":"code","source":"!pip uninstall -qy jupyterlab  # Remove unused packages from Kaggle's base image that conflict\n!pip install -U -q \"google-genai==1.7.0\"\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T09:42:54.296475Z","iopub.execute_input":"2025-04-13T09:42:54.296845Z","iopub.status.idle":"2025-04-13T09:43:03.106348Z","shell.execute_reply.started":"2025-04-13T09:42:54.296815Z","shell.execute_reply":"2025-04-13T09:43:03.105049Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Installing additional code to support the SDK","metadata":{}},{"cell_type":"code","source":"from google import genai\nfrom google.genai import types\nfrom IPython.display import HTML, Markdown, display\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T09:43:11.252399Z","iopub.execute_input":"2025-04-13T09:43:11.252768Z","iopub.status.idle":"2025-04-13T09:43:12.874248Z","shell.execute_reply.started":"2025-04-13T09:43:11.252736Z","shell.execute_reply":"2025-04-13T09:43:12.873316Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Set Up the API Key","metadata":{}},{"cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\n\nGOOGLE_API_KEY = UserSecretsClient().get_secret(\"GOOGLE_API_KEY\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T09:43:20.497676Z","iopub.execute_input":"2025-04-13T09:43:20.498276Z","iopub.status.idle":"2025-04-13T09:43:20.695838Z","shell.execute_reply.started":"2025-04-13T09:43:20.498241Z","shell.execute_reply":"2025-04-13T09:43:20.69475Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Summary of elements installed\n\nAdded image in cover for notebook.\n\nDefined the API key and confirmed it is set up correctly.\n\nInstallation of varous libaries and models for use within the notebook. For example: BertModel, VisionEncoderDecoderModel and CLIPProcessor and others.\n","metadata":{}},{"cell_type":"markdown","source":"# GEN AI CAPABILITIES EXEMPLIFIED","metadata":{}},{"cell_type":"markdown","source":"# EMBEDDING AND VECTOR SEARCH ( 1st Example)\n","metadata":{}},{"cell_type":"markdown","source":"Objective:\n\nLoading and Preprocessing the Image: The code loads an image using the PIL library and preprocesses it with ViTFeatureExtractor to prepare it for caption generation.\n\nGenerating a Caption for the Image: It uses the VisionEncoderDecoderModel to generate a caption based on the preprocessed image, leveraging a pre-trained model on image-caption pairs.\n\nDecoding and Printing the Generated Caption: The AutoTokenizer decodes the model's output into a human-readable caption, which is then printed to provide a textual description of the image.\n\n\n","metadata":{}},{"cell_type":"code","source":"from transformers import BertModel, BertTokenizer\n\ndef encode_question(question):\n    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n    model = BertModel.from_pretrained('bert-base-uncased')\n    inputs = tokenizer(question, return_tensors='pt')\n    outputs = model(**inputs)\n    return outputs.last_hidden_state\n\ndef link_question_to_graph(question_embedding, node_embeddings):\n    similarities = torch.nn.functional.cosine_similarity(question_embedding, node_embeddings)\n    relevant_nodes = torch.argmax(similarities, dim=1)\n    return relevant_nodes\n\ndef retrieve_answer(relevant_nodes, graph_data):\n    answers = []\n    for node in relevant_nodes:\n        answers.append(graph_data[node])\n    return answers\n\ndef display_answer(answer):\n    display(Markdown(answer))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T09:43:29.998337Z","iopub.execute_input":"2025-04-13T09:43:29.99868Z","iopub.status.idle":"2025-04-13T09:43:57.202839Z","shell.execute_reply.started":"2025-04-13T09:43:29.998651Z","shell.execute_reply":"2025-04-13T09:43:57.201669Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#Access Uploaded Image in Notebook","metadata":{}},{"cell_type":"code","source":"#Install Rasterio\n!pip install rasterio\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T09:44:10.516013Z","iopub.execute_input":"2025-04-13T09:44:10.517296Z","iopub.status.idle":"2025-04-13T09:44:16.561794Z","shell.execute_reply.started":"2025-04-13T09:44:10.517236Z","shell.execute_reply":"2025-04-13T09:44:16.560325Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Install Numpy\n!pip install numpy\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T09:44:30.225582Z","iopub.execute_input":"2025-04-13T09:44:30.226003Z","iopub.status.idle":"2025-04-13T09:44:34.553241Z","shell.execute_reply.started":"2025-04-13T09:44:30.225951Z","shell.execute_reply":"2025-04-13T09:44:34.552056Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from PIL import Image\n\n# Open the JPEG image\njpeg_image_path = '/kaggle/input/brazil-agriculture/MYD13A2_V061_04Aug2020_Brazil_HERO.original.jpg'\nimage = Image.open(jpeg_image_path)\n\n# Save the image as a TIFF in a writable directory\ntif_image_path = '/kaggle/working/Brazil.tiff'\nimage.save(tif_image_path, format='TIFF')\n\nprint(f\"Image converted and saved as {tif_image_path}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T10:13:36.635293Z","iopub.execute_input":"2025-04-13T10:13:36.635672Z","iopub.status.idle":"2025-04-13T10:13:36.67248Z","shell.execute_reply.started":"2025-04-13T10:13:36.635643Z","shell.execute_reply":"2025-04-13T10:13:36.671278Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import rasterio\nimport numpy as np\n\n# Load satellite image\ndef load_image(file_path):\n    with rasterio.open(file_path) as src:\n        image = src.read()\n    return image\n\n# Example usage\nfile_path = '/kaggle/working/Brazil.tiff'\nimage = load_image(file_path)\n\n# Display basic information about the image\nprint(f\"Image shape: {image.shape}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T10:14:54.046518Z","iopub.execute_input":"2025-04-13T10:14:54.047026Z","iopub.status.idle":"2025-04-13T10:14:54.061148Z","shell.execute_reply.started":"2025-04-13T10:14:54.046971Z","shell.execute_reply":"2025-04-13T10:14:54.059812Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#Preprocessing ","metadata":{}},{"cell_type":"code","source":"import rasterio\nfrom rasterio.enums import Resampling\nimport numpy as np\nfrom skimage import exposure, segmentation\n\n# Geometric Correction and Registration\ndef geometric_correction(image_path):\n    with rasterio.open(image_path) as src:\n        transform = src.transform\n        profile = src.profile\n        data = src.read(\n            out_shape=(\n                src.count,\n                int(src.height * 2),\n                int(src.width * 2)\n            ),\n            resampling=Resampling.bilinear\n        )\n        profile.update(transform=transform, height=data.shape[1], width=data.shape[2])\n        corrected_image_path = '/kaggle/working/Brazil.tiff'\n        with rasterio.open(corrected_image_path, 'w', **profile) as dst:\n            dst.write(data)\n    return corrected_image_path\n\n# Atmospheric Correction\ndef atmospheric_correction(image):\n    corrected_image = image - np.mean(image, axis=0)\n    return corrected_image\n\n# Cloud Removal\ndef cloud_removal(image):\n    cloud_mask = image < np.percentile(image, 90)\n    image[cloud_mask] = np.nan\n    return image\n\n# Normalization/Standardization\ndef normalize_image(image):\n    normalized_image = exposure.rescale_intensity(image, out_range=(0, 1))\n    return normalized_image\n\n# Image Segmentation\ndef segment_image(image):\n    segments = segmentation.slic(image, n_segments=100, compactness=10)\n    return segments\n\n# Create Bounding Boxes/Polygons\ndef create_bounding_boxes(segments):\n    bounding_boxes = []\n    for segment in np.unique(segments):\n        mask = segments == segment\n        coords = np.column_stack(np.where(mask))\n        bounding_boxes.append(coords)\n    return bounding_boxes\n\n# Example preprocessing workflow\nimage_path = '/kaggle/working/Brazil.tiff'\ncorrected_image_path = geometric_correction(image_path)\nwith rasterio.open(corrected_image_path) as src:\n    corrected_image = src.read(1)\nnormalized_image = normalize_image(corrected_image)\nsegments = segment_image(normalized_image)\nbounding_boxes = create_bounding_boxes(segments)\n\nprint(f\"Number of segments: {len(bounding_boxes)}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install transformers torch pillow","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T10:31:25.65016Z","iopub.execute_input":"2025-04-13T10:31:25.650515Z","iopub.status.idle":"2025-04-13T10:31:30.276577Z","shell.execute_reply.started":"2025-04-13T10:31:25.650482Z","shell.execute_reply":"2025-04-13T10:31:30.275137Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":" from PIL import Image\n from transformers import VisionEncoderDecoderModel, ViTFeatureExtractor, AutoTokenizer\n\n # Load the image\n image_path = '/kaggle/working/Brazil.tiff'  # Update this with the correct path\n image = Image.open(image_path)\n\n # Load the model and feature extractor\n model = VisionEncoderDecoderModel.from_pretrained(\"nlpconnect/vit-gpt2-image-captioning\")\n feature_extractor = ViTFeatureExtractor.from_pretrained(\"nlpconnect/vit-gpt2-image-captioning\")\n tokenizer = AutoTokenizer.from_pretrained(\"nlpconnect/vit-gpt2-image-captioning\")\n\n # Preprocess the image\n pixel_values = feature_extractor(images=image, return_tensors=\"pt\").pixel_values\n\n # Generate a caption\n outputs = model.generate(pixel_values)\n caption = tokenizer.decode(outputs[0], skip_special_tokens=True)\n\n print(f\"Generated Caption: {caption}\")\n ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T10:32:08.170558Z","iopub.execute_input":"2025-04-13T10:32:08.170941Z","iopub.status.idle":"2025-04-13T10:32:25.545025Z","shell.execute_reply.started":"2025-04-13T10:32:08.170908Z","shell.execute_reply":"2025-04-13T10:32:25.543124Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":" from transformers import VisualBertModel, BertTokenizer\n import torch\n\n # Load the model and tokenizer\n model = VisualBertModel.from_pretrained(\"uclanlp/visualbert-vqa-coco-pre\")\n tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n\n # Define the question and the caption (context)\n question = \"What is in the image?\"\n context = caption  # Use the generated caption as context\n\n # Tokenize the input\n inputs = tokenizer(question, context, return_tensors=\"pt\")\n\n # Forward pass\n outputs = model(**inputs)\n logits = outputs.logits\n\n # Get the answer (assuming binary classification for simplicity)\n answer = torch.argmax(logits, dim=-1).item()\n\n print(f\"Answer: {answer}\")\n ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T10:32:54.416648Z","iopub.execute_input":"2025-04-13T10:32:54.417102Z","iopub.status.idle":"2025-04-13T10:32:59.393764Z","shell.execute_reply.started":"2025-04-13T10:32:54.417068Z","shell.execute_reply":"2025-04-13T10:32:59.391098Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from PIL import Image\nfrom transformers import VisionEncoderDecoderModel, ViTFeatureExtractor, AutoTokenizer, VisualBertModel, BertTokenizer\nimport torch\n\n# Load the image\nimage_path = '/kaggle/working/Brazil.tiff'  # Update this with the correct path\nimage = Image.open(image_path)\n\n# Load the image captioning model and feature extractor\ncaption_model = VisionEncoderDecoderModel.from_pretrained(\"nlpconnect/vit-gpt2-image-captioning\")\nfeature_extractor = ViTFeatureExtractor.from_pretrained(\"nlpconnect/vit-gpt2-image-captioning\")\ncaption_tokenizer = AutoTokenizer.from_pretrained(\"nlpconnect/vit-gpt2-image-captioning\")\n\n# Preprocess the image for captioning\npixel_values = feature_extractor(images=image, return_tensors=\"pt\").pixel_values\n\n# Generate a caption\ncaption_outputs = caption_model.generate(pixel_values)\ncaption = caption_tokenizer.decode(caption_outputs[0], skip_special_tokens=True)\n\nprint(f\"Generated Caption: {caption}\")\n\n# Load the VisualBERT model and tokenizer for question answering\nqa_model = VisualBertModel.from_pretrained(\"uclanlp/visualbert-vqa-coco-pre\")\nqa_tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n\n# Define the question and the caption (context)\nquestion = \"What is in the image?\"\ncontext = caption  # Use the generated caption as context\n\n# Tokenize the input for question answering\ninputs = qa_tokenizer(question, context, return_tensors=\"pt\")\n\n# Forward pass for question answering\noutputs = qa_model(**inputs)\n\n# Access the last hidden state\nlast_hidden_state = outputs.last_hidden_state\n\n# Process the last hidden state to get the answer (example: using a simple linear layer)\nlinear_layer = torch.nn.Linear(last_hidden_state.size(-1), 2)  # Assuming binary classification\nlogits = linear_layer(last_hidden_state[:, 0, :])  # Use the first token's representation\n\n# Get the answer\nanswer = torch.argmax(logits, dim=-1).item()\n\nprint(f\"Answer: {answer}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T10:35:27.249937Z","iopub.execute_input":"2025-04-13T10:35:27.251147Z","iopub.status.idle":"2025-04-13T10:35:41.151416Z","shell.execute_reply.started":"2025-04-13T10:35:27.251095Z","shell.execute_reply":"2025-04-13T10:35:41.15013Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# FEW- SHOT PROMPTING CAPABILITY ( 2nd Example)","metadata":{}},{"cell_type":"markdown","source":"Objective\n\nDefine the Task: Define the task you want the model to perform. For this example I will be answering questions.\n\nProvide Examples: Give the model a few examples of the task. These examples should include both the input and the desired output. The more relevant and clear the examples, the better the model will understand the task.\n\nConstruct the Prompt: Combine the task definition and examples into a single prompt. This prompt will guide the model in generating the desired output.\n","metadata":{}},{"cell_type":"code","source":"from transformers import VisionEncoderDecoderModel, ViTFeatureExtractor, AutoTokenizer\nfrom PIL import Image\n\n# Load the image captioning model and feature extractor\nmodel = VisionEncoderDecoderModel.from_pretrained(\"nlpconnect/vit-gpt2-image-captioning\")\nfeature_extractor = ViTFeatureExtractor.from_pretrained(\"nlpconnect/vit-gpt2-image-captioning\")\ntokenizer = AutoTokenizer.from_pretrained(\"nlpconnect/vit-gpt2-image-captioning\")\n\n# Define the few-shot examples\nexamples = [\n    (\"/kaggle/working/Brazil.tiff\", \"An image of Agricultural Vegitation.\"),\n    (\"/kaggle/working/Brazil.tiff\", \"An image of areas of dense vegitation.\"),\n    (\"/kaggle/working/Brazil.tiff\", \"An image of areas of sparse vegitation.\")\n]\n\n# Function to generate captions using few-shot prompting\ndef generate_caption(image_path):\n    image = Image.open(image_path)\n    pixel_values = feature_extractor(images=image, return_tensors=\"pt\").pixel_values\n    outputs = model.generate(pixel_values)\n    caption = tokenizer.decode(outputs[0], skip_special_tokens=True)\n    return caption\n\n# Generate captions for the examples\nfor image_path, true_caption in examples:\n    generated_caption = generate_caption(image_path)\n    print(f\"Image: {image_path}\")\n    print(f\"True Caption: {true_caption}\")\n    print(f\"Generated Caption: {generated_caption}\")\n    print()\n\n# Generate a caption for a new image\nnew_image_path = \"/kaggle/working/Brazil.tiff\"\nnew_caption = generate_caption(new_image_path)\nprint(f\"Generated Caption for New Image: {new_caption}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T11:02:53.696157Z","iopub.execute_input":"2025-04-13T11:02:53.696622Z","iopub.status.idle":"2025-04-13T11:03:09.587259Z","shell.execute_reply.started":"2025-04-13T11:02:53.696587Z","shell.execute_reply":"2025-04-13T11:03:09.586066Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# IMAGE UNDERSTANDING ( 3rd Example)\n","metadata":{}},{"cell_type":"markdown","source":"Objective:\n\nCLIP Model: The code uses the CLIP model to understand the image and match it with textual descriptions.\n\nText Prompts: I provided 2 text prompts that describe possible contents of the image.\n\nImage Processing: The image and text prompts are processed and fed into the CLIP  model.\n\nSimilarity Scores: The model outputs similarity scores between the image and each text prompt, which are converted to probabilities.\n\n\n","metadata":{}},{"cell_type":"code","source":" import torch\n from PIL import Image\n from transformers import CLIPProcessor, CLIPModel\n\n # Load the CLIP model and processor\n model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n\n # Load the image\n image_path = '/kaggle/input/ndvi-map-image-understanding/agri- image 2.png'\n image = Image.open(image_path)\n\n # Define the text prompts\n prompts = [\"A satellite agriculture image\", \"A image of dense vegitation\"]\n\n # Preprocess the image and text\n inputs = processor(text=prompts, images=image, return_tensors=\"pt\", padding=True)\n\n # Forward pass\n outputs = model(**inputs)\n\n # Get the logits\n logits_per_image = outputs.logits_per_image  # Image-to-text similarity scores\n probs = logits_per_image.softmax(dim=1)  # Convert logits to probabilities\n\n # Print the probabilities for each prompt\n for prompt, prob in zip(prompts, probs[0]):\n     print(f\"Prompt: {prompt}, Probability: {prob.item()}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T11:21:25.176249Z","iopub.execute_input":"2025-04-13T11:21:25.176823Z","iopub.status.idle":"2025-04-13T11:21:34.734354Z","shell.execute_reply.started":"2025-04-13T11:21:25.176784Z","shell.execute_reply":"2025-04-13T11:21:34.73316Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# SUMMARY\n\nI learned a lot doing this capstone. I still have a lot to learn but I am trying.\n\nThank You\n\nDH","metadata":{}}]}